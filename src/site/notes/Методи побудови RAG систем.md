---
{"title":"Методи побудови RAG систем","dg-publish":true,"dg-metatags":null,"dg-home":null,"permalink":"/metodi-pobudovi-rag-sistem/","dgPassFrontmatter":true,"noteIcon":""}
---



Процес Retrieval-Augmented Generation (RAG) представляє собою досить складну систему, що складається з багатьох компонентів. 
#### Типові задачі процесів RAG систем

- Класифікація запитів
- Поділ на фрагменти
- Векторизація даних
- Пошук
- Переранжування
- Узагальнення даних
- Генерація відповіді

Кожен з цих етапів відіграє важливу роль у забезпеченні точності та ефективності системи. Наприклад, класифікація запитів допомагає визначити, чи потрібно взагалі виконувати пошук, що може значно скоротити час обробки. Переранжування та переупаковка документів покращують релевантність результатів, а узагальнення допомагає усунути надлишковість та покращити якість відповідей.

На мою думку, ключова складність RAG полягає в тому, що кожен етап вимагає ретельної настройки та вибору оптимальних методів. Наприклад, як вибрати розмір фрагментів для пошуку? Яку модель ембеддингу використовувати? Ці питання вимагають не лише теоретичного аналізу, а й практичних експериментів.

#### Класифікація запитів

Класифікація запитів — це перший крок, який дозволяє визначити, чи потрібен пошук взагалі. Не всі запити вимагають додаткового пошуку, особливо якщо модель великих мов (LLM) вже володіє достатніми знаннями. Наприклад, якщо запит ґрунтується виключно на інформації, наданій користувачем, пошук може бути зайвим.

**Основні типи запитів:**

- Фактичний пошук (наприклад, «Скільки коштує побудувати будинок?»)
- Думка або оцінка («Що краще цегла чи каркас?»)
- Роздум або синтез інформації («Де краще вибирати землю для будівництва»)
- Особистий або неформальний запит («Як ти думаєш, краще побудувати будинок чи жити в квартирі?»)

**Стратегії пошуку:**

- Якщо запит вимагає фактичної інформації, задіюється пошук у базі знань або векторний пошук.
- Якщо запит складний, комбінується кілька джерел інформації.
- Якщо відповідь можна дати без пошуку, LLM модель відповідає напряму.

**Методи класифікації запитів:**

- Лінгвістичний аналіз (розбір структури речення, ключових слів).
- ML-моделі класифікації (навчені на розмічених даних, наприклад, BERT або GPT).
- Правила та евристики (наприклад, наявність питальних слів або ключових фраз).

**Приклад класифікації запитів з використанням бібліотеки transformers та моделі BERT:**

```python
import torch
from transformers import pipeline

# Завантажуємо попередньо навчену модель для класифікації
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

# Можливі класи запитів
labels = ["фактичний пошук", "думка", "роздум", "особистий запит", "невизначений"]

# Функція класифікації запиту
def classify_query(query):
    result = classifier(query, candidate_labels=labels)
    return result["labels"][0], result["scores"][0]

# Приклади запитів
queries = [
    "Скільки коштує побудувати будинок?",
    "Як ти думаєш, погрузчик краще, чим екскаватор?",
    "Поясни принципи роботи ДВЗ.",
    "Варто мені переїхати в інше місто?",
    "Просто хочу поговорити."
]

# Класифікуємо запити
for query in queries:
    label, score = classify_query(query)
    print(f"Запит: {query}\nКласифікація: {label} (впевненість: {score:.2f})\n")
```

**Результат виконання:**

- Запит: Скільки коштує побудувати будинок?
  Класифікація: фактичний пошук (впевненість: 0.24)

- Запит: Як ти думаєш, погрузчик краще, чим екскаватор?
  Класифікація: особистий запит (впевненість: 0.28)

- Запит: Поясни принципи роботи ДВЗ.
  Класифікація: роздум (впевненість: 0.34)

- Запит: Варто мені переїхати в інше місто?
  Класифікація: думка (впевненість: 0.30)

- Запит: Просто хочу поговорити.
  Класифікація: роздум (впевненість: 0.26)

На мою думку, автоматизація цього процесу за допомогою класифікатора — це найкраще рішення. Однак важливо враховувати, що класифікатор повинен бути навчений на різноманітних даних, щоб уникнути помилок у реальних сценаріях.

#### Поділ на частини

Поділ документів на фрагменти — це критично важливий етап, який впливає на точність пошуку. В основі лежить три рівня фрагментації: на рівні токенів, семантична фрагментація та фрагментація на рівні речень. На мою думку, фрагментація на рівні речень є найбільш збалансованим підходом, оскільки вона зберігає семантику тексту, залишаючись при цьому досить простою для реалізації.

**Приклад фіксованого поділу на 512 токенів:**

```python
from transformers import AutoTokenizer

# Завантажуємо токенізатор (використовуємо BERT)
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def chunk_text(text, max_tokens=512):
    tokens = tokenizer.encode(text, add_special_tokens=False)
    chunks = [tokens[i:i+max_tokens] for i in range(0, len(tokens), max_tokens)]
    return [tokenizer.decode(chunk) for chunk in chunks]

# Тестовий текст
text = "Шту́чний інтеле́кт (англ. artificial intelligence; AI) в найширшому сенсі — це інтелект, демонстрований машинами, в тому числі комп'ютерними системами. Це область досліджень в галузі комп'ютерних наук, яка розробляє та вивчає методи та програмне забезпечення, що дозволяють машинам сприймати навколишнє середовище та використовувати навчання та інтелект для виконання дій, які максимально збільшують їхні шанси на досягнення поставлених цілей[1]. Такі машини можна назвати штучним інтелектом. Деякі з найвідоміших застосувань штучного інтелекту включають передові пошукові системи (наприклад, Google Search, Bing, Яндекс); рекомендаційні системи (використовувані на YouTube, Amazon та Netflix); взаємодію за допомогою людської мови (наприклад, Google Assistant, Siri, Alexa, Аліса); автономні транспортні засоби (наприклад, Waymo); генеративні та творчі інструменти (наприклад, ChatGPT, Apple Intelligence та мистецтво штучного інтелекту); а також надлюдську гру та аналіз у стратегічних іграх (наприклад, шахи та го). Однак багато застосувань штучного інтелекту не сприймаються як штучний інтелект: «Багато передових розробок штучного інтелекту проникли в загальні застосування, часто не називаючись штучним інтелектом, тому що як тільки щось стає достатньо корисним та достатньо поширеним, його вже не називають штучним інтелектом»[2][3]. Алан Тьюринг був першим, хто провів масштабні дослідження в галузі, яку він назвав машинним інтелектом[4]. Штучний інтелект був заснований як академічна дисципліна в 1956 році[5] тими, кого зараз вважають батьками-засновниками штучного інтелекту: Джоном Маккарті, Марвіном Минським, Натаніелем Рочестером[англ.] та Клодом Шенноном[6][7]. Ця галузь пережила кілька циклів оптимізму[8], за якими слідували періоди розчарування та втрати фінансування, відомі як зима штучного інтелекту[9]. Фінансування та інтерес значно зросли після 2012 року, коли глибоке навчання перевершило всі попередні методи штучного інтелекту[10], а також після 2017 року з появою архітектури Transformer[11]. Це призвело до буму штучного інтелекту на початку 2020-х років, коли компанії, університети та лабораторії, переважно базуючись у Сполучених Штатах, стали піонерами значних досягнень у галузі штучного інтелекту[12]. Зростаюче використання штучного інтелекту в XXI столітті впливає на суспільний та економічний зсув у бік більшої автоматизації, прийняття рішень на основі даних та інтеграції систем штучного інтелекту в різні сектори економіки та сфери життя, впливаючи на ринки праці, охорону здоров'я, державне управління, промисловість, освіту, пропаганду та дезінформацію. Це ставить питання про довгострокові ефекти, етичні наслідки та ризики штучного інтелекту, спонукаючи до обговорень про політику регулювання, спрямовану на забезпечення безпеки та переваг цієї технології. Різні напрямки досліджень штучного інтелекту зосереджені навколо певних цілей та використання певних інструментів. Традиційні цілі досліджень штучного інтелекту включають міркування, представлення знань, планування, навчання, обробку природної мови, сприйняття та підтримку робототехніки[13]. Загальний інтелект, або сильний, — здатність виконувати будь-яку задачу, яку може виконати людина, принаймні на рівному рівні — входить до числа довгострокових цілей цієї галузі[14]. Для досягнення цих цілей дослідники штучного інтелекту адаптували та інтегрували широкий спектр методів, включаючи пошук та математичну оптимізацію, формальну логіку, штучні нейронні мережі, а також методи, засновані на статистиці, дослідженні операцій та економіці[13]. Штучний інтелект також спирається на психологію, лінгвістику, філософію, нейронауку та інші галузі[15]."

# Розбиваємо на шматки по 512 токенів
chunks = chunk_text(text)

for i, chunk in enumerate(chunks):
    print(f"Фрагмент {i+1}:\n{chunk}\n")
```

**Результат виконання:**

- Фрагмент 1: Шту́чний інтеле́кт (англ. artificial intelligence; AI) в найширшому сенсі — це інтелект, демонстрований машинами, в тому числі комп'ютерними системами. Це область досліджень в галузі комп'ютерних наук, яка розробляє та вивчає методи та програмне забезпечення, що дозволяють машинам сприймати навколишнє середовище та використовувати навчання та інтелект для виконання дій, які максимально збільшують їхні шанси на досягнення поставлених цілей[1]. Такі машини можна назвати штучним інтелектом. Деякі з найвідоміших застосувань штучного інтелекту включають передові пошукові системи (наприклад, Google Search, Bing, Яндекс); рекомендаційні системи (використовувані на YouTube, Amazon та Netflix); взаємодію за допомогою людської мови (наприклад, Google Assistant, Siri, Alexa, Аліса); автономні транспортні засоби (наприклад, Waymo); генеративні та творчі інструменти (наприклад, ChatGPT, Apple Intelligence та мистецтво штучного інтелекту); а також надлюдську гру та аналіз у стратегічних іграх (наприклад, шахи та го). Однак багато застосувань штучного інтелекту не сприймаються як штучний інтелект: «Багато передових розробок штучного інтелекту проникли в загальні застосування, часто не називаючись штучним інтелектом, тому що як тільки щось стає достатньо корисним та достатньо поширеним, його вже не називають штучним інтелектом»[2][3]. Алан Тьюринг був першим, хто провів масштабні дослідження в галузі, яку він назвав машинним інтелектом[4].

- Фрагмент 2: Штучний інтелект був заснований як академічна дисципліна в 1956 році[5] тими, кого зараз вважають батьками-засновниками штучного інтелекту: Джоном Маккарті, Марвіном Минським, Натаніелем Рочестером[англ.] та Клодом Шенноном[6][7]. Ця галузь пережила кілька циклів оптимізму[8], за якими слідували періоди розчарування та втрати фінансування, відомі як зима штучного інтелекту[9]. Фінансування та інтерес значно зросли після 2012 року, коли глибоке навчання перевершило всі попередні методи штучного інтелекту[10], а також після 2017 року з появою архітектури Transformer[11].

- Фрагмент 3: Це призвело до буму штучного інтелекту на початку 2020-х років, коли компанії, університети та лабораторії, переважно базуючись у Сполучених Штатах, стали піонерами значних досягнень у галузі штучного інтелекту[12]. Зростаюче використання штучного інтелекту в XXI столітті впливає на суспільний та економічний зсув у бік більшої автоматизації, прийняття рішень на основі даних та інтеграції систем штучного інтелекту в різні сектори економіки та сфери життя, впливаючи на ринки праці, охорону здоров'я, державне управління, промисловість, освіту, пропаганду та дезінформацію. Це ставить питання про довгострокові ефекти, етичні наслідки та ризики штучного інтелекту, спонукаючи до обговорень про політику регулювання, спрямовану на забезпечення безпеки та переваг цієї технології. Різні напрямки досліджень штучного інтелекту зосереджені навколо певних цілей та використання певних інструментів. Традиційні цілі досліджень штучного інтелекту включають міркування, представлення знань, планування, навчання, обробку природної мови, сприйняття та підтримку робототехніки[13]. Загальний інтелект, або сильний, — здатність виконувати будь-яку задачу, яку може виконати людина, принаймні на рівному рівні — входить до числа довгострокових цілей цієї галузі[14]. Для досягнення цих цілей дослідники штучного інтелекту адаптували та інтегрували широкий спектр методів, включаючи пошук та математичну оптимізацію, формальну логіку, штучні нейронні мережі, а також методи, засновані на статистиці, дослідженні операцій та економіці[13]. Штучний інтелект також спирається на психологію, лінгвістику, філософію, нейронауку та інші галузі[15].

**Приклад семантичного поділу з використанням spaCy для розбиття тексту на смислові блоки:**

```python
import spacy

# Завантажуємо модель NLP
nlp = spacy.load("ru_core_news_sm")

def semantic_chunking(text, max_sentences=3):
    doc = nlp(text)
    sentences = [sent.text for sent in doc.sents]

    # Групуємо речення по max_sentences
    chunks = []
    for i in range(0, len(sentences), max_sentences):
        chunk = " ".join(sentences[i:i+max_sentences])
        chunks.append(chunk)

    return chunks

# Приклад тексту
text = "Шту́чний інтеле́кт (англ. artificial intelligence; AI) в найширшому сенсі — це інтелект, демонстрований машинами, в тому числі комп'ютерними системами. Це область досліджень в галузі комп'ютерних наук, яка розробляє та вивчає методи та програмне забезпечення, що дозволяють машинам сприймати навколишнє середовище та використовувати навчання та інтелект для виконання дій, які максимально збільшують їхні шанси на досягнення поставлених цілей[1]. Такі машини можна назвати штучним інтелектом. Деякі з найвідоміших застосувань штучного інтелекту включають передові пошукові системи (наприклад, Google Search, Bing, Яндекс); рекомендаційні системи (використовувані на YouTube, Amazon та Netflix); взаємодію за допомогою людської мови (наприклад, Google Assistant, Siri, Alexa, Аліса); автономні транспортні засоби (наприклад, Waymo); генеративні та творчі інструменти (наприклад, ChatGPT, Apple Intelligence та мистецтво штучного інтелекту); а також надлюдську гру та аналіз у стратегічних іграх (наприклад, шахи та го). Однак багато застосувань штучного інтелекту не сприймаються як штучний інтелект: «Багато передових розробок штучного інтелекту проникли в загальні застосування, часто не називаючись штучним інтелектом, тому що як тільки щось стає достатньо корисним та достатньо поширеним, його вже не називають штучним інтелектом»[2][3]. Алан Тьюринг був першим, хто провів масштабні дослідження в галузі, яку він назвав машинним інтелектом[4]. Штучний інтелект був заснований як академічна дисципліна в 1956 році[5] тими, кого зараз вважають батьками-засновниками штучного інтелекту: Джоном Маккарті, Марвіном Минським, Натаніелем Рочестером[англ.] та Клодом Шенноном[6][7]. Ця галузь пережила кілька циклів оптимізму[8], за якими слідували періоди розчарування та втрати фінансування, відомі як зима штучного інтелекту[9]. Фінансування та інтерес значно зросли після 2012 року, коли глибоке навчання перевершило всі попередні методи штучного інтелекту[10], а також після 2017 року з появою архітектури Transformer[11]. Це призвело до буму штучного інтелекту на початку 2020-х років, коли компанії, університети та лабораторії, переважно базуючись у Сполучених Штатах, стали піонерами значних досягнень у галузі штучного інтелекту[12]. Зростаюче використання штучного інтелекту в XXI столітті впливає на суспільний та економічний зсув у бік більшої автоматизації, прийняття рішень на основі даних та інтеграції систем штучного інтелекту в різні сектори економіки та сфери життя, впливаючи на ринки праці, охорону здоров'я, державне управління, промисловість, освіту, пропаганду та дезінформацію. Це ставить питання про довгострокові ефекти, етичні наслідки та ризики штучного інтелекту, спонукаючи до обговорень про політику регулювання, спрямовану на забезпечення безпеки та переваг цієї технології. Різні напрямки досліджень штучного інтелекту зосереджені навколо певних цілей та використання певних інструментів. Традиційні цілі досліджень штучного інтелекту включають міркування, представлення знань, планування, навчання, обробку природної мови, сприйняття та підтримку робототехніки[13]. Загальний інтелект, або сильний, — здатність виконувати будь-яку задачу, яку може виконати людина, принаймні на рівному рівні — входить до числа довгострокових цілей цієї галузі[14]. Для досягнення цих цілей дослідники штучного інтелекту адаптували та інтегрували широкий спектр методів, включаючи пошук та математичну оптимізацію, формальну логіку, штучні нейронні мережі, а також методи, засновані на статистиці, дослідженні операцій та економіці[13]. Штучний інтелект також спирається на психологію, лінгвістику, філософію, нейронауку та інші галузі[15]."

# Розбиваємо на семантичні фрагменти
chunks = semantic_chunking(text)

# Вивід результатів
for i, chunk in enumerate(chunks):
    print(f"🔹 Фрагмент {i+1}:\n{chunk}\n")
```

**Результат виконання:**

- Фрагмент 1: Шту́чний інтеле́кт (англ. artificial intelligence; AI) в найширшому сенсі — це інтелект, демонстрований машинами, в тому числі комп'ютерними системами. Це область досліджень в галузі комп'ютерних наук, яка розробляє та вивчає методи та програмне забезпечення, що дозволяють машинам сприймати навколишнє середовище та використовувати навчання та інтелект для виконання дій, які максимально збільшують їхні шанси на досягнення поставлених цілей[1]. Такі машини можна назвати штучним інтелектом.

- Фрагмент 2: Деякі з найвідоміших застосувань штучного інтелекту включають передові пошукові системи (наприклад, Google Search, Bing, Яндекс); рекомендаційні системи (використовувані на YouTube, Amazon та Netflix); взаємодію за допомогою людської мови (наприклад, Google Assistant, Siri, Alexa, Аліса); автономні транспортні засоби (наприклад, Waymo); генеративні та творчі інструменти (наприклад, ChatGPT, Apple Intelligence та мистецтво штучного інтелекту); а також надлюдську гру та аналіз у стратегічних іграх (наприклад, шахи та го). Однак багато застосувань штучного інтелекту не сприймаються як штучний інтелект: «Багато передових розробок штучного інтелекту проникли в загальні застосування, часто не називаючись штучним інтелектом, тому що як тільки щось стає достатньо корисним та достатньо поширеним, його вже не називають штучним інтелектом»[2][3]. Алан Тьюринг був першим, хто провів масштабні дослідження в галузі, яку він назвав машинним інтелектом[4].

- Фрагмент 3: Штучний інтелект був заснований як академічна дисципліна в 1956 році[5] тими, кого зараз вважають батьками-засновниками штучного інтелекту: Джоном Маккарті, Марвіном Минським, Натаніелем Рочестером[англ.] та Клодом Шенноном[6][7]. Ця галузь пережила кілька циклів оптимізму[8], за якими слідували періоди розчарування та втрати фінансування, відомі як зима штучного інтелекту[9]. Фінансування та інтерес значно зросли після 2012 року, коли глибоке навчання перевершило всі попередні методи штучного інтелекту[10], а також після 2017 року з появою архітектури Transformer[11].

Поділ на фрагменти критично впливає на якість пошуку та генерації. Вибір стратегії залежить від типу документів та завдань системи.

- **Фіксовані розміри** — підходять для великих текстів.
- **Семантичне розбиття** — дає більш осмислені шматки.
- **Структурне розбиття** — корисне для технічних даних.

Найкращий підхід — комбінувати методи в залежності від контексту!

#### Вибір моделі ембеддингу

Модель ембеддингу виконує основну функцію в пошуку релевантних документів. Досвід показує, що важливо поєднувати високу продуктивність з невеликим розміром моделі. Важливо також враховувати, що вибір моделі може залежати від конкретної задачі та доступних ресурсів.

**Як це працює?**

Модель перетворює текст у багатовимірне векторне представлення. Ці вектори потім використовуються для пошуку подібних фрагментів у базі даних.

**При виборі моделі важливо визначити:**

- **Мова та специфіка текстових даних (домен)**
  - Якщо система працює на англійській → OpenAI ada-002, BGE, SBERT
  - Для російської → DeepPavlov/RuBERT, sbert_large_nlu_ru
  - Для кодових або медичних даних → CodeBERT, BioBERT

- **Оптимальний розмір векторів та швидкість**
  - Короткі вектори (384–512) швидші, але менш точні.
  - Довгі вектори (1024+) дають точний пошук, але вимагають більше пам'яті.

- **Здатність до вилучення смислових зв'язків**
  - Косинусна подібність краще працює з SBERT, BGE
  - Для довгих текстів краще підходять E5 або Cohere

- **Сумісність з базами даних**
  - Якщо використовується FAISS/Chroma → SBERT, BGE, ada-002
  - Якщо використовується Milvus/Weaviate → OpenAI, Cohere

**Приклад векторизації даних:**

```python
from sentence_transformers import SentenceTransformer

# Завантажуємо модель
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# Приклади фрагментів тексту
texts = [
    "RAG використовує зовнішню базу знань для генерації відповідей.",
    "Моделі встраивання допомагають шукати релевантну інформацію.",
    "Глибокі нейронні мережі покращують обробку природної мови."
]

# Встраюємо текст у вектори
embeddings = model.encode(texts)

# Виводимо розмір вектора
print(f"Розмір вектора: {len(embeddings[0])}")
print(f"Приклад вектора: {embeddings[0][:5]}")  # Виведемо перші 5 значень
```

**Результат виконання:**

- Розмір вектора: 384
- Приклад вектора: [-0.00326376 0.07672146 0.01198601 -0.01142005 -0.04499646]

#### Вибір векторної бази даних

Векторна база даних відповідає за зберігання та швидкий пошук релевантних фрагментів інформації. Вибір підходящої бази даних впливає на швидкість роботи, точність пошуку та масштабованість системи.

**Що робить векторна база даних?**

- Зберігання текстових фрагментів у вигляді багатовимірних векторів.
- Пошук найбільш подібних векторів за запитом.
- Використання індексів для швидкого пошуку (ANN — Approximate Nearest Neighbors).

**Основні критерії при виборі:**

1. **Розмір даних та масштабованість**
   - Невеликий обсяг даних → FAISS, Chroma
   - Якщо потрібен масштабований пошук → Weaviate, Milvus, Qdrant

2. **Швидкість пошуку**
   - Якщо важлива швидка обробка → FAISS, ScaNN (Google)
   - Якщо важливий онлайн-доступ та REST API → Weaviate, Pinecone

3. **Підтримка гібридного пошуку (векторний + класичний full-text)**
   - Weaviate, Qdrant, Vespa дозволяють комбінувати векторний та текстовий пошук.

4. **Простота інтеграції**
   - FAISS, Chroma — легко використовувати в Python.
   - Pinecone, Weaviate, Qdrant — зручні для хмарних рішень.

**Приклад пошуку схожих векторів:**

```python
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# Завантажуємо модель встраивання
model = SentenceTransformer("all-MiniLM-L6-v2")

# Ісходні текстові фрагменти
texts = [
    "Як працює RAG?",
    "Векторні бази даних зберігають embeddings.",
    "Машинне навчання покращує пошук інформації.",
    "Глибокі нейромережі використовуються в NLP."
]

# Перетворюємо текст у вектори
embeddings = model.encode(texts)
dimension = embeddings.shape[1]  # Визначаємо розмірність векторів

# Створюємо індекс FAISS
index = faiss.IndexFlatL2(dimension)
index.add(np.array(embeddings))  # Додаємо вектори в базу

# Запит (теж перетворюємо в вектор)
query = "Як працюють векторні бази?"
query_embedding = model.encode([query])

# Шукаємо 2 найближчих вектора
D, I = index.search(np.array(query_embedding), k=2)

# Виводимо результати
print("Знайдені фрагменти:")
for idx in I[0]:
    print(f"- {texts[idx]}")
```

**Результат виконання:**

- Знайдені фрагменти:
  - Як працює RAG?
  - Машинне навчання покращує пошук інформації.

#### Організація пошуку та переранжування

Методи пошуку, такі як генерація запитів, значно покращують релевантність результатів. Однак слід враховувати, що ці методи можуть бути ресурсомісткими, що важливо враховувати в реальних застосуваннях.

1. **Пошук (Retrieval)**
   На цьому етапі система виконує векторний пошук або гібридний пошук (комбінація векторного та класичного текстового пошуку).

**Методи пошуку:**

- **Векторний пошук (ANN – Approximate Nearest Neighbors):** порівняння ембеддингу запиту та бази (FAISS, Weaviate, Pinecone).
- **Класичний пошук (BM25):** пошук за ключовими словами (Elasticsearch, Weaviate, Qdrant).
- **Гібридний пошук (BM25 + ANN):** поєднання двох методів для підвищення точності.

2. **Переранжування (Re-ranking)**
   Після пошуку отримані документи ранжуються за релевантністю. Прості методи (BM25) можуть повертати нерелевантні результати, тому використовується доранжування (re-ranking) за допомогою нейронних мереж.

**Моделі ранжування:**

- **Cross-Encoder (SBERT)** – донавчена модель для визначення найбільш релевантних фрагментів.
- **Cohere Rerank** – хмарна модель для покращення пошуку.
- **OpenAI reranking (GPT)** – доранжування за допомогою GPT.

**Приклад пошуку та переранжування:**

```python
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer, CrossEncoder

# Завантажуємо моделі
retrieval_model = SentenceTransformer("all-MiniLM-L6-v2")  # Для пошуку
reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")  # Для переранжування

# Ісходні текстові фрагменти
documents = [
    "LLM можуть генерирувати відповіді на основі знайдених даних.",
    "Пошук інформації можливий через BM25 та векторний пошук.",
    "Глибокі нейромережі допомагають аналізувати текст.",
    "RAG використовує векторні бази даних для пошуку."
]

# Перетворюємо документи в ембеддинги
doc_embeddings = retrieval_model.encode(documents)
dimension = doc_embeddings.shape[1]

# Створюємо FAISS індекс
index = faiss.IndexFlatL2(dimension)
index.add(np.array(doc_embeddings))

# Запит користувача
query = "Як працює пошук в RAG?"
query_embedding = retrieval_model.encode([query])

# Пошук 3 найближчих фрагментів
D, I = index.search(np.array(query_embedding), k=3)
retrieved_docs = [documents[idx] for idx in I[0]]

# Переранжування знайдених документів
scores = reranker.predict([(query, doc) for doc in retrieved_docs])
ranked_docs = [doc for _, doc in sorted(zip(scores, retrieved_docs), reverse=True)]

# Вивід результатів
print("Знайдені документи (після переранжування):")
for doc in ranked_docs:
    print(f"- {doc}")
```

**Приклад виконання:**

- Знайдені документи (після переранжування):
  - RAG використовує векторні бази даних для пошуку.
  - Пошук інформації можливий через BM25 та векторний пошук.
  - Глибокі нейромережі допомагають аналізувати текст.

#### Реалізація резюмування та генерації відповіді

Резюмування вилучених документів — це етап, який допомагає усунути надлишковість та покращити якість відповідей. Рекомендується поєднувати екстрактивні та генеративні підходи, такий підхід забезпечує високу точність та ефективність.

Після того, як система вилучила фрагменти з нашої бази знань та провела їх ранжування, відбувається обробка тексту для виділення ключових ідей:

- **Удалення несуттєвих деталей:** Наприклад, можуть бути видалені зайві слова або надлишкова інформація, яка не впливає на відповідь.
- **Предобробка тексту:** Може включати переформулювання або спрощення фрагментів, щоб зробити інформацію більш доступною для сприйняття.

**Підготовка контексту для генерації:**
Після вилучення релевантних документів та фрагментів, ці дані формують контекст, який використовується для генерації відповіді.

- **Контекст включає** як інформацію з вилучених документів, так і сам запит користувача. Важливо, щоб система правильно зіставила запит з потрібними фрагментами з документів.
- В деяких випадках цей контекст додатково структурується або очищається, щоб виключити зайві або невідповідні дані, які можуть заважати генерації.

**Генеративна модель** створює відповідь на основі оброблених фрагментів.

- Генеративна модель може взяти інформацію з багатьох фрагментів та «стиснути» її в лаконічну та точну відповідь. Краще за все для таких завдань підходять моделі, які здатні зв'язно генерирувати текст на основі наданого контексту.
- Моделі можуть використовувати механізми уваги (attention) для «запам'ятовування» ключових частин контексту, необхідних для коректної відповіді.

**Останнім етапом узагальнення є узгодження.** Важливо, щоб усі дані в відповіді були логічно поєднані та не суперечили одне одному. Реалізація повинна відповідати принципам референтності, коли моделі повинні стежити, щоб посилання на факти були точними, а також узгодженості, коли відповідь повинна бути логічно вибудованою, щоб не було різких стрибків думок.

**Моделі для резюмування в RAG-системах:**

- **T5 (Text-to-Text Transfer Transformer)** - Модель, яка навчена на багатьох текстових завданнях та може виконувати завдання генерації тексту, включаючи резюмування.
- **BART (Bidirectional and Auto-Regressive Transformers)** - Модель для стискаючого та абстрактного резюмування. Працює як енкодер-декодер, вилучаючи інформацію з тексту та генерируючи логічно вибудований відповідь.

**Приклад простого резюмування:**

```python
from transformers import pipeline

# Завантажуємо модель для резюмування (BART)
summarizer = pipeline("summarization", model="sberbank-ai/rugpt3small_sum")

# Приклад тексту
text = """
У Retrieval-Augmented Generation (RAG) використовується дві ключові технології: вилучення інформації та генерація відповіді.
Система спочатку знаходить релевантні фрагменти з бази даних, а потім генерує на основі цих фрагментів пов'язаний відповідь.
Процес резюмування допомагає зробити відповіді більш лаконічними, зменшуючи обсяг непотрібної інформації та представляючи лише основні факти.
"""

# Генерація резюме
summary = summarizer(text, max_length=50, min_length=25, do_sample=False)

# Вивід резюме
print("Обобщений текст:", summary[0]['summary_text'])
```

**Результат виконання:**

- Обобщений текст: У Retrieval-Augmented Generation (RAG) використовується вилучення інформації та генерація відповіді. Резюмування допомагає зменшити обсяг інформації та представити лише основні факти.

При виборі стратегій резюмування в RAG-системах слід опиратися на їх особливості:

- **Стискаюче резюмування (Extractive Summarization)** система вибирає суттєві фрагменти з вихідного тексту, не змінюючи їх, та об'єднує їх для формування відповіді. Такий підхід простіший, але обмежує гнучкість.
- **Абстрактне резюмування (Abstractive Summarization)** модель переформульовує вихідні дані своїми словами, що дає більше гнучкості та можливість створювати більш зв'язні та логічні відповіді. Це складніше, але результат часто більш точний та корисний.

Вибір стратегій побудови RAG систем лежить між максимальною продуктивністю, що включає всі модулі для досягнення максимальної точності, та збалансованою ефективністю, яка оптимізує продуктивність та ефективність, виключаючи деякі ресурсомісткі методи.

На мою думку, вибір стратегії залежить від конкретних вимог проекту. Наприклад, якщо важлива швидкість обробки, можна пожертвувати деякою точністю на користь ефективності. Однак у завданнях, де точність критично важлива, краще використовувати всі модулі.

У цій статті я звернув увагу на кілька важливих ідей:

1. **Важливість модульної конструкції:** Оптимізація кожного компонента окремо дозволяє створювати гнучкі та легко настроювані системи. Це особливо важливо в складних проектах, де вимоги можуть змінюватися з часом.
2. **Систематичний підхід до практичної реалізації:** наведений досвід демонструє, як ретельне тестування на загальновизнаних наборах даних може забезпечити надійність результатів. Наведені приклади реалізації можуть стати в нагоді при виборі та аналізі етапів RAG.
3. **Існуючі практичні обмеження:** Незважаючи на всі переваги RAG, існують проблеми, такі як узагальнення на приватні дані, продуктивність у реальному часі та інтеграція мультимодальних даних. Ці питання вимагають додаткового вивчення при побудові подібних систем.

На мою думку, запропоновані методи та стратегії можуть значно покращити якість та ефективність систем RAG. Однак важливо пам'ятати, що кожне завдання унікальне, і успішне впровадження RAG вимагає не лише дотримання найкращих практик, а й адаптації до конкретних умов.

---

**Теги:** #AI, #rag, #data, #db

**Хаби:** [Штучний інтелект](https://habr.com/ru/hubs/artificial_intelligence/), [Data Engineering](https://habr.com/ru/hubs/data_engineering/)

---

**Коментарі:**

- [Коментар 1](https://habr.com/ru/articles/881268/comments/#comment_1)
- [Коментар 2](https://habr.com/ru/articles/881268/comments/#comment_2)
- [Коментар 3](https://habr.com/ru/articles/881268/comments/#comment_3)
- [Коментар 4](https://habr.com/ru/articles/881268/comments/#comment_4)

---

**Події:**

- [Epic Telegram Conference](https://habr.com/ru/events/576/) (11–13 лютого)
- [Вебінар «Практикум Cloud.ru Evolution: як з'єднати кілька віртуальних машин»](https://habr.com/ru/events/580/) (13 лютого)
- [«Чесні вакансії: DevOps Middle»: безкоштовний вебінар від навчального центру Слёрм](https://habr.com/ru/events/578/) (13 лютого)
- [Онлайн-інтенсив «Нейромережі для HR у 2025 році»](https://habr.com/ru/events/584/) (14–19 лютого)
- [Конференція True Tech Hub у рамках фестивалю «Система Фест»](https://habr.com/ru/events/574/) (15 лютого)
- [Весенній Хабр Семінар: як синергія HR і PR допомагає утримувати співробітників](https://habr.com/ru/events/582/) (13 березня)
- [Deckhouse Conf 2025](https://habr.com/ru/events/572/) (27 березня)
- [IT-конференція Merge Tatarstan 2025](https://habr.com/ru/events/516/) (25–26 квітня)

---

**Соціальні мережі:**

- [Facebook](https://www.facebook.com/habrahabr.ru)
- [Twitter](https://twitter.com/habr_com)
- [VK](https://vk.com/habr)
- [Telegram](https://telegram.me/habr_com)
- [YouTube](https://www.youtube.com/channel/UCd_sTwKqVrweTt4oAKY5y4w)
- [Яндекс Дзен](https://dzen.ru/habr)

---

**Технічна підтримка:** [Налаштування](https://habr.com/ru/feedback/)

**© 2006–2025, [Habr](https://company.habr.com/)**