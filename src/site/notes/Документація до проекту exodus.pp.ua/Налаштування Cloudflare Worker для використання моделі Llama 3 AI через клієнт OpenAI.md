---
{"title":"Налаштування Cloudflare Worker для використання моделі Llama 3 AI через клієнт OpenAI","dg-publish":true,"dg-metatags":null,"dg-home":null,"permalink":"/dokumentacziya-do-proektu-exodus-pp-ua/nalashtuvannya-cloudflare-worker-dlya-vikoristannya-modeli-llama-3-ai-cherez-kliyent-open-ai/","dgPassFrontmatter":true,"noteIcon":""}
---



 
## Передумови

Перед початком переконайтеся, що у вас є наступне:

- **Обліковий запис Cloudflare**: Якщо у вас його ще немає, зареєструйтеся на [сайті Cloudflare](https://www.cloudflare.com/).
- **Встановлений інструмент Wrangler**: Це інструмент командного рядка для роботи з Cloudflare Workers. Встановити його можна за інструкціями на [офіційному сайті](https://developers.cloudflare.com/workers/cli-wrangler/).

---

## Крок 0. Конфігурація

На цьому етапі ми підготуємо базові налаштування для Cloudflare Worker.

### 1. Редагування файлу `wrangler.toml`

- **Що це за файл?** Файл `wrangler.toml` — це конфігураційний файл для вашого Cloudflare Worker. У ньому вказані основні налаштування, такі як ім'я воркера.
- **Що потрібно зробити?**
  1. Відкрийте файл `wrangler.toml` у вашому проекті за допомогою будь-якого текстового редактора (наприклад, VS Code, Notepad++).
  2. Знайдіть рядок, де вказано ім'я воркера (наприклад, `name = "my-worker"`).
  3. Якщо ви хочете використовувати інше ім'я, змініть його (наприклад, `name = "my-ai-worker"`).
- **Для чого це потрібно?** Ім'я воркера — це унікальний ідентифікатор, який використовується для розгортання та управління вашим воркером у Cloudflare.

### 2. Створення KV простору імен

- **Що таке KV простір імен?** KV (Key-Value) — це сховище даних у Cloudflare, де можна зберігати пари ключ-значення. Воно потрібне для роботи з деякими функціями воркера, наприклад, для збереження даних асистентів.
- **Що потрібно зробити?**
  1. Відкрийте термінал (командний рядок) на вашому комп'ютері.
  2. Виконайте наступні команди для створення двох KV просторів імен:
     ```bash
     wrangler kv:namespace create kv
     wrangler kv:namespace create kv --preview
     ```
  3. Після виконання команд ви отримаєте ідентифікатори KV просторів імен (наприклад, `id = "abc123"`).
  4. Відкрийте файл `wrangler.toml` і додайте ці ідентифікатори у відповідні розділи:
     ```toml
     kv_namespaces = [
       { binding = "KV", id = "ваш_ідентифікатор", preview_id = "ваш_прев'ю_ідентифікатор" }
     ]
     ```
- **Для чого це потрібно?** KV простір імен необхідний для зберігання даних, які використовуються вашим воркером. Наприклад, якщо ви створюєте асистента, його дані зберігатимуться в KV.

---

## Крок 1. Розгортання

На цьому етапі ми розгорнемо Cloudflare Worker і підготуємо його до роботи.

### 1. Створення файлу `.env`

- **Що таке файл `.env`?** Це файл, де зберігаються змінні середовища (наприклад, ключі API, URL-адреси). Вони використовуються для безпечного доступу до вашого воркера.
- **Що потрібно зробити?**
  1. У вашому проекті знайдіть файл `.env.example` (цей файл зазвичай надається як шаблон).
  2. Скопіюйте його до нового файлу `.env` за допомогою команди:
     ```bash
     cp .env.example .env
     ```
  3. Відкрийте файл `.env` у текстовому редакторі та переконайтеся, що він містить потрібні змінні (наприклад, `CLOUDFLARE_WORKER_URL` та `API_KEY`). Ми повернемося до їх заповнення на кроці тестування.
- **Для чого це потрібно?** Файл `.env` дозволяє безпечно зберігати конфіденційні дані, які використовуватимуться під час тестування та роботи воркера.

### 2. Встановлення залежностей

- **Що таке залежності?** Це пакети (бібліотеки), які потрібні для роботи вашого проєкту.
- **Що потрібно зробити?**
  1. У терміналі виконайте команду:
     ```bash
     pnpm i
     ```
  2. Якщо у вас не встановлений `pnpm`, спочатку встановіть його за допомогою команди:
     ```bash
     npm install -g pnpm
     ```
- **Для чого це потрібно?** Команда `pnpm i` встановлює всі необхідні пакети, зазначені у файлі `package.json`, щоб ваш воркер міг працювати.

### 3. Розгортання воркера

- **Що таке розгортання?** Це процес завантаження вашого воркера на сервери Cloudflare, щоб він став доступним для використання.
- **Що потрібно зробити?**
  1. У терміналі виконайте команду:
     ```bash
     pnpm run deploy
     ```
  2. Після виконання команди ви отримаєте URL-адресу вашого воркера (наприклад, `https://my-ai-worker.youraccount.workers.dev`).
- **Для чого це потрібно?** Розгортання дозволяє зробити ваш воркер доступним у мережі, щоб ви могли використовувати його для викликів AI.

### 4. Створення API ключа

- **Що таке API ключ?** Це унікальний ідентифікатор, який використовується для аутентифікації запитів до вашого воркера.
- **Що потрібно зробити?**
  1. У терміналі виконайте команду:
     ```bash
     pnpm run api-key
     ```
  2. Після виконання команди ви отримаєте API ключ, який потрібно буде використовувати під час тестування.
- **Для чого це потрібно?** API ключ забезпечує безпечний доступ до вашого воркера, щоб сторонні особи не могли використовувати його без дозволу.

---

## Крок 2. Додавання додаткових облікових даних

На цьому етапі ми додамо секрети, які дозволять автоматично отримувати список доступних моделей AI.

### 1. Додавання секретів до Cloudflare Workers

- **Що таке секрети?** Це конфіденційні дані (наприклад, ключі API), які зберігаються в Cloudflare Workers у зашифрованому вигляді.
- **Що потрібно зробити?**
  1. Знайдіть ваш Cloudflare API ключ та ідентифікатор облікового запису:
     - API ключ можна отримати в розділі "API Tokens" на панелі керування Cloudflare.
     - Ідентифікатор облікового запису (Account ID) знаходиться в розділі "Workers" або на головній сторінці вашого облікового запису.
  2. У терміналі виконайте наступні команди, замінивши `Your Cloudflare API Key/Token` та `Your Cloudflare Account ID` на ваші значення:
     ```bash
     wrangler secret put CF_API_KEY <<<"Your Cloudflare API Key/Token"
     wrangler secret put CF_ACCOUNT_ID <<<"Your Cloudflare Account ID"
     ```
- **Для чого це потрібно?** Додавання цих секретів дозволяє вашому воркеру автоматично отримувати список доступних моделей AI. Якщо ви не додасте ці секрети, буде доступний лише обмежений список моделей, визначений у файлі `src/models.ts`.

---

## Крок 3. Тестування

На цьому етапі ми протестуємо ваш Cloudflare Worker, щоб переконатися, що він працює правильно.

### 1. Налаштування файлу `.env`

- **Що потрібно зробити?**
  1. Відкрийте файл `.env`, який ви створили на Кроці 1.
  2. Переконайтеся, що в ньому є такі змінні:
     - `CLOUDFLARE_WORKER_URL`: URL-адреса вашого воркера (наприклад, `https://my-ai-worker.youraccount.workers.dev/v1`).
     - `API_KEY`: API ключ, який ви згенерували на Кроці 1.
  3. Якщо ви працюєте в режимі розробки:
     - Ви можете використовувати локальний URL, наприклад, `localhost:3000/v1`.
     - У файлі `wrangler.toml` додайте налаштування `[dev] host = "dev-worker.example.com"`.
     - Використовуйте Cloudflare Tunnel для експонування воркера локально.
- **Для чого це потрібно?** Правильне налаштування `.env` дозволяє тестувати ваш воркер із правильними URL та ключами доступу.

### 2. Тестування ендпойнтів

- **Що таке ендпойнти?** Це URL-адреси, які використовуються для взаємодії з вашим воркером (наприклад, для отримання списку моделей або створення чату).
- **Що потрібно зробити?**
  1. У вашому проекті є папка `scripts`, яка містить готові скрипти для тестування різних ендпойнтів.
  2. Кожен скрипт містить інструкції та список змінних середовища, які потрібно вказати у файлі `.env` або експортувати в терміналі.

---

## Розбір таблиці ендпойнтів

Тут ми детально розберемо кожен ендпойнт із таблиці, пояснимо, для чого він потрібен, і як його протестувати.

### 1. `/models/search` (GET)
- **Опис**: Повертає список усіх доступних моделей AI без необхідності аутентифікації.
- **Використання**: Відкрийте URL у браузері (наприклад, `https://my-ai-worker.youraccount.workers.dev/models/search`), щоб отримати HTML-виведення.
- **Для чого це потрібно?** Це простий спосіб переглянути доступні моделі без використання скриптів.

### 2. `/models/search?query=json` (GET)
- **Опис**: Повертає список усіх доступних моделей у форматі JSON без необхідності аутентифікації.
- **Використання**: Відкрийте URL у браузері (наприклад, `https://my-ai-worker.youraccount.workers.dev/models/search?query=json`), щоб отримати JSON-виведення.
- **Для чого це потрібно?** JSON-формат зручний для програмного використання, наприклад, у скриптах або додатках.

### 3. `/v1/models` (GET)
- **Опис**: Повертає список усіх доступних моделей AI.
- **Скрипт для тестування**:
  ```bash
  scripts/models.sh
  ```
- **Для чого це потрібно?** Цей ендпойнт використовується для отримання повного списку моделей, якщо додані секрети `CF_API_KEY` та `CF_ACCOUNT_ID`.

### 4. `/v1/chat/completions` (POST)
- **Опис**: Створює завершення чату (наприклад, відповідь на ваше повідомлення).
- **Скрипт для тестування**:
  ```bash
  scripts/chat-completion.sh
  ```
- **Для чого це потрібно?** Це основний ендпойнт для роботи з чатами, наприклад, для створення діалогів із моделлю Llama 3.

### 5. `/v1/chat/completions` (POST, потокове завершення)
- **Опис**: Створює потокове завершення чату (відповідь передається частинами в реальному часі).
- **Скрипт для тестування**:
  ```bash
  scripts/chat-completion-stream.sh
  ```
- **Для чого це потрібно?** Потокове завершення дозволяє отримувати відповіді поступово, що зручно для довгих відповідей.

### 6. `/v1/embeddings` (POST)
- **Опис**: Створює вбудовування (вектори для тексту). Включає 3+2 тести.
- **Скрипт для тестування**:
  ```bash
  scripts/embeddings.sh
  ```
- **Для чого це потрібно?** Вбудовування використовуються для аналізу тексту, наприклад, для пошуку схожих документів.

### 7. `/v1/assistants` (POST)
- **Опис**: Створює нового асистента. Дані зберігаються в KV сховищі.
- **Скрипт для тестування**:
  ```bash
  scripts/assistants-create.sh
  ```
- **Для чого це потрібно?** Асистенти — це спеціальні об'єкти, які можна використовувати для збереження контексту або налаштувань.

### 8. `/v1/assistants` (GET)
- **Опис**: Повертає список усіх асистентів.
- **Скрипт для тестування**:
  ```bash
  scripts/assistants-list.sh
  ```
- **Для чого це потрібно?** Дозволяє переглянути всіх створених асистентів.

### 9. `/v1/assistants/:id` (GET)
- **Опис**: Повертає інформацію про конкретного асистента.
- **Скрипт для тестування**:
  ```bash
  ASST_ID=asst_abc scripts/assistant-retrieve.sh
  ```
- **Для чого це потрібно?** Дозволяє отримати деталі про конкретного асистента за його ID.

### 10. `/v1/assistants/:id` (POST)
- **Опис**: Модифікує існуючого асистента.
- **Скрипт для тестування**:
  ```bash
  ASST_ID=asst_abc scripts/assistant-modify.sh
  ```
- **Для чого це потрібно?** Дозволяє змінювати налаштування асистента.

### 11. `/v1/assistants/:id` (DELETE)
- **Опис**: Видаляє асистента.
- **Скрипт для тестування**:
  ```bash
  ASST_ID=asst_abc scripts/assistant-delete.sh
  ```
- **Для чого це потрібно?** Дозволяє видалити непотрібного асистента.

### 12. `/v1/threads` (POST)
- **Опис**: Створює новий потік (ланцюжок повідомлень).
- **Для чого це потрібно?** Потоки використовуються для збереження контексту діалогів.

### 13. `/v1/threads/:id` (GET)
- **Опис**: Повертає інформацію про конкретний потік.
- **Для чого це потрібно?** Дозволяє переглянути деталі потоку.

### 14. `/v1/threads/:id` (POST)
- **Опис**: Модифікує існуючий потік.
- **Для чого це потрібно?** Дозволяє змінювати налаштування потоку.

### 15. `/v1/threads/:id` (DELETE)
- **Опис**: Видаляє потік.
- **Для чого це потрібно?** Дозволяє видалити непотрібний потік.

### 16. `/v1/threads/:id/runs` (POST)
- **Опис**: Створює запуск для потоку (виконання запиту).
- **Для чого це потрібно?** Дозволяє запустити обробку потоку.

### 17. `/v1/threads/:id/runs` (POST, потоковий режим)
- **Опис**: Створює запуск для потоку з потокуванням.
- **Для чого це потрібно?** Дозволяє отримувати результати поступово.

---

### Відповідь на запитання

Так, у проекті [https://github.com/pa4080/openai-to-cloudflare-ai.git](https://github.com/pa4080/openai-to-cloudflare-ai.git) є ендпоінти, які можна використати для вирішення проблем із обмеженнями довжини відповіді та часу виконання запитів у безкоштовному білінговому плані Cloudflare AI. На основі цього проекту та вашої реалізації оптимізованоого Docker-контейнера для архітектури ARM64 з інтеграцією Cloudflare AI Workers ([документація](https://exodus.pp.ua/dokumentacziya-do-proektu-exodus-pp-ua/optimizovanij-docker-kontejner-wrangler-dlya-arhitekturi-arm64-z-integracziyeyu-cloudflare-ai-workers/)), я розробив методику подолання цих обмежень. Нижче наведено детальний аналіз і рекомендації.

---

### Обмеження безкоштовного плану Cloudflare AI

Безкоштовний план Cloudflare AI має певні обмеження, які впливають на роботу з ендпоінтами:

1. **Обмеження довжини відповіді**: Максимальна кількість токенів або символів у відповіді від моделі AI може бути обмежена.
2. **Обмеження часу виконання**: Якщо запит займає занадто багато часу, він може бути перерваний.
3. **Обмеження кількості запитів**: Можлива наявність ліміту на кількість запитів за певний період (хоча це не вказано прямо у вашому запиті, це типове обмеження).

Проект `openai-to-cloudflare-ai` дозволяє адаптувати виклики до Cloudflare Workers через інтерфейс, схожий на OpenAI API, що відкриває можливості для обходу цих обмежень.

---

### Придатні ендпоінти

На основі README проекту, наступні ендпоінти можуть бути використані для вирішення зазначених проблем:

1. **`/v1/chat/completions` (потоковий режим)**:
   - **Опис**: Цей ендпоінт підтримує потокове завершення чату (streamed chat completion) з параметром `stream: true`.
   - **Використання**: Див. скрипт `scripts/chat-completion-stream.sh`.
   - **Перевага**: Потоковий режим дозволяє отримувати відповідь частинами, що допомагає уникнути перевищення обмеження довжини відповіді.

2. **`/v1/chat/completions` (звичайний режим)**:
   - **Опис**: Створює завершення чату у стандартному режимі.
   - **Використання**: Див. скрипт `scripts/chat-completion.sh`.
   - **Перевага**: Можна налаштувати параметри (наприклад, максимальну кількість токенів), щоб відповідь відповідала обмеженням.

3. **`/v1/models`**:
   - **Опис**: Повертає список доступних моделей AI.
   - **Використання**: Див. скрипт `scripts/models.sh`.
   - **Перевага**: Дозволяє обрати менш ресурсоємну модель, що може зменшити час виконання та розмір відповіді.

---

### Методика подолання обмежень

Ось покрокова методика подолання обмежень довжини відповіді та часу виконання на основі проекту `openai-to-cloudflare-ai` та вашої Docker-реалізації:

#### 1. Використання потокового режиму
- **Проблема**: Обмеження довжини відповіді.
- **Рішення**: Використовуйте ендпоінт `/v1/chat/completions` із параметром `stream: true`. Це дозволяє отримувати відповідь частинами в реальному часі, уникаючи перевищення ліміту на розмір одного запиту.
- **Реалізація в Docker**:
  - Налаштуйте ваш додаток у Docker-контейнері для відправки запитів із параметром `stream: true`.
  - Використовуйте скрипт `chat-completion-stream.sh` як приклад для інтеграції.
- **Приклад запиту**:
  ```bash
  curl -X POST "https://your-worker.workers.dev/v1/chat/completions" \
       -H "Authorization: Bearer YOUR_API_KEY" \
       -d '{"model": "llama-3", "messages": [{"role": "user", "content": "Довгий запит..."}], "stream": true}'
  ```

#### 2. Розбиття великих запитів
- **Проблема**: Обмеження часу виконання.
- **Рішення**: Розбийте великі запити на менші частини (наприклад, за абзацами чи реченнями) і надсилайте їх послідовно через `/v1/chat/completions`.
- **Реалізація в Docker**:
  - Додайте логіку розбиття тексту в вашому мікросервері перед відправкою запитів.
  - Обробляйте відповіді послідовно та об'єднуйте їх за потреби.
- **Приклад**: Якщо текст має 1000 слів, розбийте його на 5 запитів по 200 слів і обробіть кожен окремо.

#### 3. Кешування відповідей
- **Проблема**: Повторні запити та потенційне перевищення кількості запитів.
- **Рішення**: Використовуйте KV-сховище Cloudflare Workers для кешування відповідей. Проект уже підтримує KV (див. конфігурацію `wrangler.toml`).
- **Реалізація в Docker**:
  - Налаштуйте кешування на рівні вашого мікросервера в Docker. Перед відправкою запиту перевіряйте, чи є відповідь у кеші.
  - Додайте логіку в код воркера для збереження відповідей у KV:
    ```javascript
    await KV.put(requestKey, response, { expirationTtl: 3600 }); // Кеш на 1 годину
    ```
- **Перевага**: Зменшує кількість запитів до Cloudflare AI.

#### 4. Оптимізація параметрів запитів
- **Проблема**: Великі відповіді та довгий час виконання.
- **Рішення**: Налаштуйте параметри в запитах до `/v1/chat/completions`, наприклад:
  - Обмежте максимальну кількість токенів у відповіді (`max_tokens`).
  - Використовуйте менш ресурсоємні моделі (перегляньте список через `/v1/models`).
- **Реалізація в Docker**:
  - Додайте параметри до запитів у вашому коді:
    ```json
    {
      "model": "llama-3",
      "messages": [{"role": "user", "content": "Короткий запит"}],
      "max_tokens": 100
    }
    ```

#### 5. Моніторинг і аналітика
- **Проблема**: Виявлення проблемних запитів.
- **Рішення**: Використовуйте Cloudflare AI Gateway або інструменти моніторингу Workers для відстеження використання ресурсів.
- **Реалізація в Docker**:
  - Інтегруйте логи та метрики у ваш контейнер для аналізу часу виконання та розміру відповідей.
  - Використовуйте команди `wrangler` для перегляду логів:
    ```bash
    wrangler tail
    ```

---

### Інтеграція з Docker для мікросервера на ARM64

Ваша реалізація Docker-контейнера з інтеграцією Cloudflare AI Workers може бути оптимізована так:

1. **Налаштування середовища**:
   - Переконайтеся, що ваш Dockerfile включає всі залежності (`pnpm`, `wrangler`) і коректно працює на ARM64.
   - Використовуйте змінні середовища з `.env` для налаштування `CLOUDFLARE_WORKER_URL` та `API_KEY`.

2. **Потоковий режим**:
   - Інтегруйте логіку для роботи з `/v1/chat/completions` у потоковому режимі у вашому мікросервері.

3. **Кешування**:
   - Реалізуйте локальний кеш у контейнері (наприклад, за допомогою Redis) або використовуйте KV Cloudflare через API Workers.

4. **Моніторинг**:
   - Додайте інструменти моніторингу (наприклад, Prometheus) у Docker для відстеження продуктивності.

---

### Висновок

Проект `openai-to-cloudflare-ai` пропонує ендпоінти, такі як `/v1/chat/completions` (з потоковим режимом) і `/v1/models`, які придатні для вирішення проблем із обмеженнями довжини відповіді та часу виконання у безкоштовному плані Cloudflare AI. Методика, що включає потоковий режим, розбиття запитів, кешування, оптимізацію параметрів і моніторинг, дозволяє ефективно обійти ці обмеження. Ваша Docker-реалізація на ARM64 може бути вдосконалена шляхом інтеграції цих підходів, що забезпечить стабільну роботу мікросервера з Cloudflare AI Workers.

Ця стратегія базується на можливостях проекту та вашій документації, забезпечуючи практичний підхід до оптимізації.