---
{"title":"Стратегія обходу обмежень Cloudflare AI","dg-publish":true,"dg-metatags":null,"dg-home":null,"permalink":"/dokumentacziya-do-proektu-exodus-pp-ua/strategiya-obhodu-obmezhen-cloudflare-ai/","dgPassFrontmatter":true,"noteIcon":""}
---


### Ключові моменти
- Найкраща стратегія — це каскадні запити з автоматичним продовженням, де Cloudflare Worker додає маркер `[TRUNCATED]`, якщо відповідь обрізається, а n8n обробляє це, надсилаючи подальші запити для завершення.
- Потрібно змінити Cloudflare Worker, щоб він додавав маркер при обрізанні, і налаштувати цикл в n8n для повторних запитів.
- Цей підхід дозволяє обійти обмеження довжини відповіді на безкоштовному плані, забезпечуючи повну відповідь через кілька ітерацій.

### Загальний опис
Ви можете ефективно вирішити проблему обмеження довжини відповіді LLM на безкоштовному плані Cloudflare AI, використовуючи n8n для автоматизації каскадних запитів. Цей метод передбачає модифікацію вашого Cloudflare Worker для виявлення обрізаних відповідей і їх маркування, а потім використання n8n для повторних запитів, щоб зібрати повну відповідь. Це особливо корисно, якщо ви працюєте з генерацією довгих текстів, де стандартна відповідь може бути обрізаною.

### Несподівана деталь
Цікаво, що обмеження довжини відповіді в Cloudflare AI, ймовірно, пов’язане з кількістю токенів (одиниць тексту, які модель може обробити), а не з кількістю символів, як ви могли припустити. Це означає, що точне керування токенами може бути важливим для оптимізації.

---

### Докладний звіт

#### Вступ
Обмеження довжини відповіді LLM на безкоштовному плані Cloudflare AI може ускладнити обробку великих текстів або генерацію довгих відповідей. Ви запропонували кілька стратегій, включаючи розділення вхідних даних, каскадні запити з автоматичним продовженням, паралельну обробку та гібридний підхід. Після аналізу, найкращою стратегією є каскадні запити з автоматичним продовженням, де Cloudflare Worker додає маркер `[TRUNCATED]` при обрізанні відповіді, а n8n використовується для повторних запитів, щоб зібрати повну відповідь. Цей звіт детально описує, як реалізувати цей підхід, включаючи необхідні зміни в Cloudflare Worker і налаштування робочого процесу в n8n.

#### Аналіз запропонованих стратегій

##### 1. Розділення вхідних даних (Input Chunking)
Ви запропонували використовувати ноди `Code` або `Split Out` в n8n для поділу вхідного тексту на частини, надсилати кожен чанк окремо до Cloudflare AI і потім об’єднати результати. Однак, враховуючи, що обмеження стосується довжини відповіді, а не вхідного тексту, цей підхід більше підходить для великих вхідних даних, а не для вирішення проблеми обрізаних відповідей. Тому він не є пріоритетним для вашої задачі.

##### 2. Каскадні запити з автоматичним продовженням
Ця стратегія передбачає модифікацію Cloudflare Worker для додавання маркера `[TRUNCATED]`, якщо відповідь обрізається, і налаштування циклу в n8n для перевірки цього маркера та надсилання подальших запитів для продовження. Наприклад, якщо відповідь містить `[TRUNCATED]`, ви видобуваєте останній фрагмент і надсилаєте новий запит з промптом, наприклад, "Продовж з цього місця: [останній_фрагмент]". Цей підхід безпосередньо вирішує проблему обрізаних відповідей і є найбільш відповідним.

##### 3. Паралельна обробка через декілька Worker-ів
Ви також запропонували розбити задачу на підзапити (наприклад, "Згенеруй план", потім "Розкрий пункт X") і використовувати паралельне гілкування в n8n. Це корисно для складних завдань, але не вирішує проблему обмеження довжини однієї відповіді, тому не є ключовим для вашої задачі.

##### 4. Гібридний підхід
Комбінування методів, таких як автоматичне продовження для генерації тексту та розділення вхідних даних для аналізу великих документів, може бути корисним, але для вашої основної проблеми (обмеження довжини відповіді) достатньо каскадних запитів.

#### Технічні деталі реалізації

##### Модифікація Cloudflare Worker
Щоб реалізувати каскадні запити, потрібно змінити ваш Cloudflare Worker, щоб він додавав маркер `[TRUNCATED]`, якщо відповідь перевищує певну довжину. Ось приклад коду:

```javascript
const maxLength = 1000; // Визначте максимальну довжину відповіді

async function runAI(model, prompt) {
  // Ваш код виклику AI тут
  // Припустимо, 'response' — це рядок, повернутий AI
  return response;
}

addEventListener('fetch', event => {
  event.respondWith(handleRequest(event.request));
});

async function handleRequest(request) {
  const prompt = await request.text();
  let response = await runAI(model, prompt);
  if (response.length >= maxLength) {
    response = response.slice(0, maxLength) + " [TRUNCATED]";
  }
  return new Response(response);
}
```

Цей код перевіряє, чи перевищує відповідь `maxLength`, і якщо так, додає маркер `[TRUNCATED]`. Зверніть увагу, що `maxLength` тут задано як кількість символів, але, як виявлено, обмеження може бути пов’язане з токенами. Для точності краще було б враховувати ліміт токенів, але якщо точний ліміт невідомий, використання символів як проксі є прийнятним.

##### Налаштування робочого процесу в n8n
В n8n потрібно створити робочий процес, який буде:
1. Надсилати початковий запит до Cloudflare Worker.
2. Перевіряти, чи містить відповідь маркер `[TRUNCATED]`.
3. Якщо так, видобувати останній фрагмент перед маркером, створювати новий промпт для продовження і надсилати ще один запит.
4. Повторювати цей процес, поки маркер не зникне, і об’єднувати всі частини відповіді.

Ось детальний опис вузлів:

| Вузол                  | Опис                                                                 |
|-----------------------|----------------------------------------------------------------------|
| Початковий вузол      | Початок робочого процесу.                                            |
| HTTP Request (Початковий) | Метод: POST, URL:

```
https://your-worker-id.workers.dev`, Тіло: `{{ $input.prompt }}`, Вихід: `response`. |
| Set Variable (Початкова відповідь) | Змінна: `current_response`, Значення: `{{ $node.Initial Request.data }}`. |
| Loop (Продовження)     | Умова: `{{ $node.current_response.data.indexOf("[TRUNCATED]") != -1 }}`. |
| Function (Видобути останній фрагмент) | Код: <br> ```javascript<br> const response = this.getItemValue('current_response');<br> const index = response.indexOf("[TRUNCATED]");<br> if (index != -1) {<br>   const last_fragment = response.substring(index - 100, index);<br>   return last_fragment;<br> } else {<br>   return "";<br> }<br> ```<br> Вихід: `last_fragment`. |
| Function (Створити промпт для продовження) | Код: <br> ```javascript<br> const last_fragment = this.getItemValue('last_fragment');<br> return "Продовж з цього місця: " + last_fragment;<br> ```<br> Вихід: `continuation_prompt`. |
| HTTP Request (Продовження) | Метод: POST, URL: `https://your-worker-id.workers.dev`, Тіло: `{{ $node.continuation_prompt }}`, Вихід: `new_response`. |
| Function (Оновити поточну відповідь) | Код: <br> ```javascript<br> const current_response = this.getItemValue('current_response');<br> const new_response = this.getItemValue('new_response');<br> let cleaned_new_response = new_response.replace("[TRUNCATED]", "");<br> const updated_response = current_response + cleaned_new_response;<br> return updated_response;<br> ```<br> Вихід: `updated_response`, встановити `current_response` як `updated_response`. |
| Set Variable (Фінальна відповідь) | Змінна: `final_response`, Значення: `{{ $node.current_response }}`. |
| Кінцевий вузол        | Вивести `final_response`. |
```
Цей робочий процес забезпечує, що відповідь буде зібрана повністю, навіть якщо вона обрізається на кожному етапі.

#### Додаткові міркування
- **Контекст для продовження**: Використання лише останнього фрагмента може не забезпечити достатнього контексту для AI, що може вплинути на згуртованість відповіді. Альтернативою є включення частини попередньої відповіді для контексту, але це може призвести до перевищення ліміту вхідних токенів.
- **Обмеження кількості ітерацій**: Щоб уникнути нескінченних циклів, додайте ліміт на кількість ітерацій (наприклад, 10) або час очікування між запитами, щоб уникнути перевищення лімітів швидкості.
- **Токени vs. Символи**: Обмеження, ймовірно, пов’язане з кількістю токенів, а не символів. Документація Cloudflare AI вказує на різні ліміти токенів для моделей, наприклад, для `@cf/meta/llama-2-7b-chat-fp16` ліміт становить 256 токенів для непотокових запитів і 2500 для потокових ([Limits · Cloudflare Workers AI docs](https://developers.cloudflare.com/workers-ai/platform/limits/)). Якщо можливо, краще враховувати токени, але якщо це складно, використання символів як проксі є прийнятним.

#### Висновок
Рекомендована стратегія — каскадні запити з автоматичним продовженням, реалізована через модифікацію Cloudflare Worker для додавання маркера `[TRUNCATED]` і налаштування циклу в n8n для повторних запитів. Це забезпечить повну відповідь, обходячи обмеження довжини на безкоштовному плані. Переконайтеся, що враховуєте можливі проблеми з контекстом і лімітом ітерацій для стабільної роботи.