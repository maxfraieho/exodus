---
{"title":"Стратегія обходу обмежень Cloudflare AI","dg-publish":true,"dg-metatags":null,"dg-home":null,"permalink":"/dokumentacziya-do-proektu-exodus-pp-ua/strategiya-obhodu-obmezhen-cloudflare-ai/","dgPassFrontmatter":true,"noteIcon":""}
---


повністю переформатуй текст в markdown для публікації в 11ty: 
### Ключові моменти
- **Найкраща стратегія** — це каскадні запити з автоматичним продовженням: Cloudflare Worker додає маркер `[TRUNCATED]`, якщо відповідь обрізається, а n8n обробляє це, надсилаючи подальші запити для завершення тексту.
- **Необхідні зміни**: Потрібно модифікувати Cloudflare Worker, щоб він додавав маркер при обрізанні, і налаштувати цикл у n8n для повторних запитів.
- **Переваги підходу**: Дозволяє обійти обмеження довжини відповіді на безкоштовному плані Cloudflare AI, забезпечуючи повну відповідь через кілька ітерацій.

### Загальний опис
Ви можете ефективно вирішити проблему обмеження довжини відповіді мовної моделі (LLM) на безкоштовному плані Cloudflare AI, використовуючи n8n для автоматизації каскадних запитів. Цей метод передбачає:
1. Модифікацію Cloudflare Worker для виявлення обрізаних відповідей і додавання маркера `[TRUNCATED]`.
2. Налаштування n8n для повторних запитів, щоб зібрати повну відповідь частинами.

Такий підхід особливо корисний при генерації довгих текстів, коли стандартна відповідь обрізається через обмеження платформи.

### Несподівана деталь
Цікаво, що обмеження довжини відповіді в Cloudflare AI залежить від кількості **токенів** (одиниць тексту, які обробляє модель), а не від кількості символів, як можна було б припустити. Це означає, що для оптимізації процесу важливо точно керувати токенами.

---

### Докладний звіт

#### Вступ
Обмеження довжини відповіді LLM на безкоштовному плані Cloudflare AI ускладнює роботу з великими текстами чи генерацію довгих відповідей. Серед розглянутих стратегій (розділення вхідних даних, каскадні запити, паралельна обробка, гібридний підхід) найкращою виявилася **каскадні запити з автоматичним продовженням**. Цей метод використовує Cloudflare Worker для маркування обрізаних відповідей маркером `[TRUNCATED]` і n8n для їхньої подальшої обробки через повторні запити. У звіті детально описано реалізацію цього підходу.

#### Аналіз запропонованих стратегій

##### 1. Розділення вхідних даних (Input Chunking)
- **Опис**: Використання вузлів `Code` або `Split Out` у n8n для поділу вхідного тексту на частини, обробка кожного чанка через Cloudflare AI та об’єднання результатів.
- **Аналіз**: Цей метод підходить для великих вхідних даних, але не вирішує проблему обрізання відповідей, оскільки обмеження стосується вихідного тексту.
- **Висновок**: Не є пріоритетним для даної задачі.

##### 2. Каскадні запити з автоматичним продовженням
- **Опис**: Cloudflare Worker додає маркер `[TRUNCATED]` при обрізанні відповіді, а n8n перевіряє його наявність і надсилає додаткові запити (наприклад, "Продовж з цього місця: [останній_фрагмент]").
- **Аналіз**: Безпосередньо вирішує проблему обрізаних відповідей, дозволяючи зібрати повний текст через ітерації.
- **Висновок**: Найкращий вибір для задачі.

##### 3. Паралельна обробка через кілька Worker-ів
- **Опис**: Розбиття задачі на підзапити (наприклад, "Згенеруй план", "Розкрий пункт X") із паралельним виконанням у n8n.
- **Аналіз**: Корисно для складних завдань, але не усуває обмеження довжини однієї відповіді.
- **Висновок**: Не ключове рішення.

##### 4. Гібридний підхід
- **Опис**: Комбінація автоматичного продовження та розділення вхідних даних.
- **Аналіз**: Може бути корисним для комплексних задач, але для основної проблеми достатньо каскадних запитів.
- **Висновок**: Надмірно складний для поточної мети.

#### Технічні деталі реалізації

##### Модифікація Cloudflare Worker
Для реалізації каскадних запитів Cloudflare Worker має додавати маркер `[TRUNCATED]`, якщо відповідь перевищує заданий ліміт. Ось приклад коду:

```javascript
const maxLength = 1000; // Максимальна довжина відповіді в символах

async function runAI(model, prompt) {
  // Ваш код для виклику AI
  // Припустимо, 'response' — це відповідь від AI
  return response;
}

addEventListener('fetch', event => {
  event.respondWith(handleRequest(event.request));
});

async function handleRequest(request) {
  const prompt = await request.text();
  let response = await runAI(model, prompt);
  if (response.length >= maxLength) {
    response = response.slice(0, maxLength) + " [TRUNCATED]";
  }
  return new Response(response);
}
```

- **Примітка**: `maxLength` задано в символах, але реальне обмеження може бути в токенах. Якщо точний ліміт токенів невідомий, символи використовуються як проксі.

##### Налаштування робочого процесу в n8n
У n8n створюється робочий процес для:
1. Надсилання початкового запиту.
2. Перевірки наявності `[TRUNCATED]`.
3. Видобування останнього фрагмента та створення нового промпта.
4. Повторення до отримання повної відповіді.

**Схема вузлів:**

| Вузол                          | Опис                                                                                     |
|-------------------------------|-----------------------------------------------------------------------------------------|
| **Start**                     | Початок робочого процесу.                                                              |
| **HTTP Request (Initial)**    | Метод: POST, URL: `https://your-worker-id.workers.dev`, Тіло: `{{ $input.prompt }}`.   |
| **Set (Initial Response)**    | Змінна: `current_response`, Значення: `{{ $node["Initial Request"].data }}`.           |
| **Loop (Continuation)**       | Умова: `{{ $node["current_response"].data.indexOf("[TRUNCATED]") != -1 }}`.            |
| **Function (Extract Fragment)** | Код: <br> ```javascript<br> const response = this.getItemValue('current_response');<br> const index = response.indexOf("[TRUNCATED]");<br> if (index != -1) {<br>   return response.substring(index - 100, index);<br> } else {<br>   return "";<br> }<br> ```<br> Вихід: `last_fragment`. |
| **Function (Create Prompt)**  | Код: <br> ```javascript<br> const last_fragment = this.getItemValue('last_fragment');<br> return "Продовж з цього місця: " + last_fragment;<br> ```<br> Вихід: `continuation_prompt`. |
| **HTTP Request (Continuation)** | Метод: POST, URL: `https://your-worker-id.workers.dev`, Тіло: `{{ $node["continuation_prompt"] }}`. |
| **Function (Update Response)** | Код: <br> ```javascript<br> const current_response = this.getItemValue('current_response');<br> const new_response = this.getItemValue('new_response');<br> let cleaned_new_response = new_response.replace("[TRUNCATED]", "");<br> return current_response + cleaned_new_response;<br> ```<br> Вихід: `updated_response`. |
| **Set (Final Response)**      | Змінна: `final_response`, Значення: `{{ $node["current_response"] }}`.                 |
| **End**                       | Виведення `final_response`.                                                            |

Цей процес гарантує повне збирання відповіді.

#### Додаткові міркування
- **Контекст**: Використання лише останнього фрагмента може порушити згуртованість тексту. Додавання більшого контексту може перевищити ліміт вхідних токенів.
- **Ліміт ітерацій**: Рекомендується обмежити кількість циклів (наприклад, до 10) для уникнення нескінченних запитів.
- **Токени vs. символи**: Обмеження залежить від токенів (наприклад, 256 для `@cf/meta/llama-2-7b-chat-fp16` у непотоковому режимі). Точний підрахунок токенів кращий, але символи — прийнятний замінник.

#### Висновок
Найкраще рішення — каскадні запити з маркером `[TRUNCATED]` у Cloudflare Worker і циклом у n8n. Це дозволяє отримати повну відповідь, обходячи обмеження безкоштовного плану. Важливо врахувати контекст і ліміт ітерацій для стабільності.

#### Ключові цитати
- [Limits · Cloudflare Workers AI docs](https://developers.cloudflare.com/workers-ai/platform/limits/)
- [Pricing · Cloudflare Workers AI](https://developers.cloudflare.com/workers-ai/platform/pricing/)
- [Streaming Enhancements](https://blog.cloudflare.com/workers-ai-streaming/)

```markdown
---
title: "Обхід ліміту відповіді в Cloudflare Workers AI за допомогою n8n"
date: 2025-02-22
tags:
  - automation
  - cloudflare
  - n8n
  - ai
description: "Як використовувати каскадні запити та автоматизацію в n8n для ефективного оброблення обмежень довжини відповіді в Cloudflare Workers AI."
---

## Ключові моменти

- **Найкраща стратегія** — це каскадні запити з автоматичним продовженням: Cloudflare Worker додає маркер `[TRUNCATED]`, якщо відповідь обрізається, а n8n обробляє це, надсилаючи подальші запити для завершення тексту.
- **Необхідні зміни**: Потрібно модифікувати Cloudflare Worker, щоб він додавав маркер при обрізанні, і налаштувати цикл у n8n для повторних запитів.
- **Переваги підходу**: Дозволяє обійти обмеження довжини відповіді на безкоштовному плані Cloudflare AI, забезпечуючи повну відповідь через кілька ітерацій.

---

## Загальний опис

Ви можете ефективно вирішити проблему обмеження довжини відповіді мовної моделі (LLM) на безкоштовному плані Cloudflare AI, використовуючи n8n для автоматизації каскадних запитів. Цей метод передбачає:

1. Модифікацію Cloudflare Worker для виявлення обрізаних відповідей і додавання маркера `[TRUNCATED]`.
2. Налаштування n8n для повторних запитів, щоб зібрати повну відповідь частинами.

Такий підхід особливо корисний при генерації довгих текстів, коли стандартна відповідь обрізається через обмеження платформи.

---

## Несподівана деталь

Цікаво, що обмеження довжини відповіді в Cloudflare AI залежить від кількості **токенів** (одиниць тексту, які обробляє модель), а не від кількості символів, як можна було б припустити. Це означає, що для оптимізації процесу важливо точно керувати токенами.

---

## Докладний звіт

### Вступ

Обмеження довжини відповіді LLM на безкоштовному плані Cloudflare AI ускладнює роботу з великими текстами чи генерацію довгих відповідей. Серед розглянутих стратегій (розділення вхідних даних, каскадні запити, паралельна обробка, гібридний підхід) найкращою виявилася **каскадні запити з автоматичним продовженням**. Цей метод використовує Cloudflare Worker для маркування обрізаних відповідей маркером `[TRUNCATED]` і n8n для їхньої подальшої обробки через повторні запити. Нижче описано реалізацію цього підходу.

---

### Аналіз запропонованих стратегій

#### 1. Розділення вхідних даних (Input Chunking)

- **Опис**: Використання вузлів `Code` або `Split Out` у n8n для поділу вхідного тексту на частини, обробка кожного чанка через Cloudflare AI та об’єднання результатів.
- **Аналіз**: Цей метод підходить для великих вхідних даних, але не вирішує проблему обрізання відповідей, оскільки обмеження стосується вихідного тексту.
- **Висновок**: Не є пріоритетним для даної задачі.

#### 2. Каскадні запити з автоматичним продовженням

- **Опис**: Cloudflare Worker додає маркер `[TRUNCATED]` при обрізанні відповіді, а n8n перевіряє його наявність і надсилає додаткові запити (наприклад, "Продовж з цього місця: [останній_фрагмент]").
- **Аналіз**: Безпосередньо вирішує проблему обрізаних відповідей, дозволяючи зібрати повний текст через ітерації.
- **Висновок**: Найкращий вибір для задачі.

#### 3. Паралельна обробка через кілька Worker-ів

- **Опис**: Розбиття задачі на підзапити (наприклад, "Згенеруй план", "Розкрий пункт X") із паралельним виконанням у n8n.
- **Аналіз**: Корисно для складних завдань, але не усуває обмеження довжини однієї відповіді.
- **Висновок**: Не ключове рішення.

#### 4. Гібридний підхід

- **Опис**: Комбінація автоматичного продовження та розділення вхідних даних.
- **Аналіз**: Може бути корисним для комплексних задач, але для основної проблеми достатньо каскадних запитів.
- **Висновок**: Надмірно складний для поточної мети.

---

### Технічні деталі реалізації

#### Модифікація Cloudflare Worker

Для реалізації каскадних запитів Cloudflare Worker має додавати маркер `[TRUNCATED]`, якщо відповідь перевищує заданий ліміт. Ось приклад коду:

```
const maxLength = 1000; // Максимальна довжина відповіді в символах

async function runAI(model, prompt) {
  // Ваш код для виклику AI
  // Припустимо, 'response' — це відповідь від AI
  return response;
}

addEventListener('fetch', event => {
  event.respondWith(handleRequest(event.request));
});

async function handleRequest(request) {
  const prompt = await request.text();
  let response = await runAI(model, prompt);
  if (response.length >= maxLength) {
    response = response.slice(0, maxLength) + " [TRUNCATED]";
  }
  return new Response(response);
}
```

- **Примітка**: `maxLength` задано в символах, але реальне обмеження може бути в токенах. Якщо точний ліміт токенів невідомий, символи використовуються як проксі.

#### Налаштування робочого процесу в n8n

У n8n створюється робочий процес для:

1. Надсилання початкового запиту.
2. Перевірки наявності `[TRUNCATED]`.
3. Видобування останнього фрагмента та створення нового промпта.
4. Повторення до отримання повної відповіді.

**Схема вузлів:**

| Вузол                          | Опис                                                                                     |
|-------------------------------|-----------------------------------------------------------------------------------------|
| **Start**                     | Початок робочого процесу.                                                              |
| **HTTP Request (Initial)**    | Метод: POST, URL: `https://your-worker-id.workers.dev`, Тіло: `{{ $input.prompt }}`.   |
| **Set (Initial Response)**    | Змінна: `current_response`, Значення: `{{ $node["Initial Request"].data }}`.           |
| **Loop (Continuation)**       | Умова: `{{ $node["current_response"].data.indexOf("[TRUNCATED]") != -1 }}`.            |
| **Function (Extract Fragment)** | Код: <br> ``````<br> Вихід: `last_fragment`. |
| **Function (Create Prompt)**  | Код: <br> `````` <br> Вихід: `continuation_prompt`. |
| **HTTP Request (Continuation)** | Метод: POST, URL: `https://your-worker-id.workers.dev`, Тіло: `{{ $node["continuation_prompt"] }}`. |
| **Function (Update Response)** | Код: <br> `````` <br> Вихід: `updated_response`. |
| **Set (Final Response)**      | Змінна: `final_response`, Значення: `{{ $node["current_response"] }}`.                 |
| **End**                       | Виведення `final_response`.                                                            |

Цей процес гарантує повне збирання відповіді.

---

### Додаткові міркування

- **Контекст**: Використання лише останнього фрагмента може порушити згуртованість тексту. Додавання більшого контексту може перевищити ліміт вхідних токенів.
- **Ліміт ітерацій**: Рекомендується обмежити кількість циклів (наприклад, до 10) для уникнення нескінченних запитів.
- **Токени vs. символи**: Реальне обмеження залежить від токенів. Для точнішої обробки можна використовувати бібліотеки токенізації.

---

## Висновок

Найкраще рішення — каскадні запити з маркером `[TRUNCATED]` у Cloudflare Worker і циклом у n8n. Це дозволяє отримати повну відповідь, обходячи обмеження безкоштовного плану. Важливо врахувати контекст і ліміт ітерацій для стабільності.

---

## Додаткові ресурси

- [Cloudflare Workers AI: Limits](https://developers.cloudflare.com/workers-ai/platform/limits/)
- [Cloudflare Workers AI: Pricing](https://developers.cloudflare.com/workers-ai/platform/pricing/)
- [Streaming Enhancements](https://blog.cloudflare.com/workers-ai-streaming/)
```

